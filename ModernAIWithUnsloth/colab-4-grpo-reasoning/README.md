# üß† Colab 4: GRPO Reasoning Model Training

## üéØ Project Goal

This notebook demonstrates training reasoning models using GRPO (Group Relative Policy Optimization) with Unsloth.ai. Unlike traditional RL (Colab 3) that uses pre-labeled preferences, GRPO generates multiple reasoning traces for problems and uses group-relative rewards to train models capable of chain-of-thought reasoning similar to OpenAI's o1.

This approach enables models to "think before answering" by generating intermediate reasoning steps, leading to better performance on complex problems.

## üìπ Video Demonstration
https://www.youtube.com/

## üèóÔ∏è Architecture Overview

### GRPO vs Traditional RL

**Traditional DPO (Colab 3):**
- Requires pre-labeled chosen/rejected pairs
- Human annotations needed
- Fixed preference data
- No reasoning trace generation

**GRPO (This Colab):**
- Generates multiple solution attempts
- Model creates own reasoning traces
- Automatic reward assignment
- Group-relative comparison
- Enables chain-of-thought capabilities

### Reasoning Model Concept

**What is a Reasoning Model?**
- Generates intermediate thinking steps
- Shows work before final answer
- Can self-correct during reasoning
- Better at complex multi-step problems
- Similar to OpenAI o1, DeepSeek R1

**Chain-of-Thought (CoT):**
- Step-by-step problem solving
- Explicit reasoning process
- Improved accuracy on hard problems
- Interpretable decision making

---

## üìã Implementation Steps

### Step 1: Environment Setup
- Install Unsloth with GRPO dependencies
- Import GRPO trainer and utilities
- Set up reasoning evaluation tools
- Verify GPU availability

### Step 2: Load Model for Reasoning
- Choose capable base model (recommend 1B+ params)
- Configure for generation tasks
- Set up special reasoning tokens
- Enable longer context for reasoning traces

### Step 3: Prepare Problem Dataset
- Load dataset with problems and solutions
- Suitable datasets:
  - GSM8K (math word problems)
  - MATH dataset
  - Code problems (HumanEval)
  - Logic puzzles
- Format: problem ‚Üí solution (no reasoning traces needed)

### Step 4: Configure GRPO Training
- Set number of generations per problem (e.g., 4-8)
- Configure reward function (correctness-based)
- Set group size for relative comparison
- Configure temperature for diverse generations
- Set GRPO-specific hyperparameters

### Step 5: Generate Reasoning Traces
- Model generates multiple attempts per problem
- Each attempt includes reasoning steps
- Diversity encouraged through temperature
- Traces evaluated for correctness
- Best traces identified automatically

### Step 6: GRPO Training Execution
- Train using group-relative rewards
- Compare generations within each group
- Reinforce better reasoning patterns
- Penalize poor reasoning
- Monitor reasoning quality metrics

### Step 7: Evaluation and Testing
- Test reasoning capabilities
- Compare with base model
- Evaluate on complex problems
- Analyze reasoning trace quality
- Measure accuracy improvements

---

## üéì Key Concepts Explained

### Group Relative Policy Optimization (GRPO)

**Core Mechanism:**
- Generate N solutions for each problem
- Evaluate each solution (correct/incorrect)
- Compute group-relative rewards
- Reinforce better solutions
- Penalize worse solutions

**Group-Relative Rewards:**
- Compare solutions within same problem
- Normalize rewards relative to group
- Reduces reward variance
- More stable training
- Better credit assignment

**Advantages over DPO:**
- No human preference labels needed
- Automatic reward from correctness
- Generates own training data
- Scales to more problems
- Enables reasoning capabilities

### Reasoning Trace Generation

**Trace Structure:**
- Problem statement
- Thinking process (step-by-step)
- Intermediate calculations
- Final answer
- Self-verification (optional)

**Quality Indicators:**
- Logical coherence
- Correct intermediate steps
- Accurate final answer
- Clear explanations
- Error detection and correction

### Reward Functions for Reasoning

**Correctness-Based:**
- Binary: correct (1) vs incorrect (0)
- Partial credit for partial solutions
- Bonus for efficient reasoning
- Penalty for incorrect logic

**Process-Based:**
- Reward intermediate steps
- Encourage clear explanations
- Penalize logical errors
- Bonus for self-correction

---

## üöÄ Usage Examples

### Example 1: Math Word Problem

**Problem:**
"Sarah has 3 times as many apples as Tom. Together they have 24 apples. How many does Sarah have?"

**Base Model (No Reasoning):**
"Sarah has 18 apples."

**GRPO Model (With Reasoning):**
"Let me think through this step by step:
1. Let Tom have x apples
2. Then Sarah has 3x apples
3. Together: x + 3x = 24
4. So 4x = 24
5. Therefore x = 6
6. Sarah has 3 √ó 6 = 18 apples
Answer: 18 apples"

### Example 2: Logic Puzzle

**Problem:**
"If all roses are flowers and some flowers fade quickly, can we conclude that some roses fade quickly?"

**Base Model:**
"Yes, some roses fade quickly."

**GRPO Model:**
"Let me analyze this logically:
1. All roses are flowers (roses ‚äÇ flowers)
2. Some flowers fade quickly (‚àÉ flowers that fade)
3. But we don't know if those specific flowers are roses
4. The fading flowers could be non-roses
5. Therefore, we cannot conclude that some roses fade quickly
Answer: No, we cannot make that conclusion."

### Example 3: Code Problem

**Problem:**
"Write a function to find the longest palindromic substring."

**Base Model:**
Provides code without explanation

**GRPO Model:**
"Let me think about the approach:
1. Need to check all possible substrings
2. For each substring, verify if palindrome
3. Track the longest one found
4. Optimization: expand around centers
5. Time complexity: O(n¬≤)

Here's the implementation:
[code with comments explaining each step]"

---

## üìä Training Metrics

### Expected GRPO Performance

| Metric | Base Model | After GRPO |
|--------|------------|------------|
| Problem Accuracy | 40-50% | 65-80% |
| Reasoning Quality | Low | High |
| Training Time | - | 30-60 min |
| Generations/Problem | 1 | 4-8 |
| Memory Usage | - | ~2-3 GB |

### Key Metrics to Monitor

**Generation Diversity:**
- Temperature controls diversity
- Higher temp = more varied attempts
- Need balance: diverse but reasonable
- Monitor unique solution approaches

**Reward Distribution:**
- Percentage of correct solutions
- Should improve during training
- Track best/worst/average rewards
- Monitor reward variance

**Reasoning Quality:**
- Step coherence
- Logical correctness
- Explanation clarity
- Self-correction frequency

---

## üí° Best Practices

### Dataset Selection
- Start with clear-cut correct/incorrect problems
- Math problems work well (verifiable answers)
- Code problems with test cases
- Logic puzzles with definite solutions
- Avoid subjective problems initially

### GRPO Configuration
- Generate 4-8 solutions per problem
- Use temperature 0.7-1.0 for diversity
- Group size = number of generations
- Learning rate: 1e-6 to 5e-6
- Train on 1000+ problems

### Reward Function Design
- Clear correctness criteria
- Partial credit when appropriate
- Bonus for efficient reasoning
- Penalty for logical errors
- Consider process rewards

### Evaluation Strategy
- Test on held-out problems
- Measure accuracy improvement
- Evaluate reasoning quality
- Check for reasoning shortcuts
- Human evaluation of traces

---

## üêõ Troubleshooting

### Issue: Model Not Generating Reasoning
**Solutions:**
- Add reasoning examples in prompts
- Use model with reasoning capabilities
- Increase generation length
- Adjust temperature
- Provide reasoning format instructions

### Issue: Low Reward Variance
**Solutions:**
- Increase generation temperature
- Generate more solutions per problem
- Use more diverse problems
- Adjust reward function
- Check for reward hacking

### Issue: Reasoning Quality Poor
**Solutions:**
- Start with simpler problems
- Provide better reasoning examples
- Increase model size
- Train for more steps
- Improve reward function

### Issue: Training Instability
**Solutions:**
- Reduce learning rate
- Smaller batch size
- More gradient accumulation
- Clip gradients
- Normalize rewards

---

## üîó Resources

### Documentation
- Unsloth GRPO Guide: https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/tutorial-train-your-own-reasoning-model-with-grpo
- Unsloth R1 Blog: https://unsloth.ai/blog/r1-reasoning
- GRPO Concepts: https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide

### Datasets
- GSM8K: https://huggingface.co/datasets/gsm8k
- MATH: https://huggingface.co/datasets/hendrycks/math
- HumanEval: https://huggingface.co/datasets/openai_humaneval

### Related Models
- OpenAI o1: https://openai.com/o1/
- DeepSeek R1: https://github.com/deepseek-ai/DeepSeek-R1
- Reasoning Models Overview: https://huggingface.co/blog/reasoning-models

---

## ‚úÖ Completion Checklist

- [ ] Environment setup with GRPO dependencies
- [ ] Model loaded (1B+ params recommended)
- [ ] Problem dataset prepared (GSM8K or similar)
- [ ] GRPO configuration set (generations, temperature)
- [ ] Reward function defined (correctness-based)
- [ ] Training completed successfully
- [ ] Multiple reasoning traces generated per problem
- [ ] Reward distribution analyzed
- [ ] Accuracy improved vs base model
- [ ] Reasoning quality evaluated
- [ ] Before/after comparison conducted
- [ ] Model generates clear reasoning steps
- [ ] Reasoning model saved
- [ ] Video walkthrough recorded covering:
  - [ ] GRPO concept explanation
  - [ ] Reasoning model capabilities
  - [ ] Training process
  - [ ] Reasoning trace examples
  - [ ] Accuracy improvements
  - [ ] Comparison with o1/R1 concepts
- [ ] Colab notebook uploaded and runnable

---

## üìù Video Walkthrough Requirements

Your video should cover:

1. **Introduction (2 min)**
   - What are reasoning models?
   - Difference from standard LLMs
   - GRPO vs DPO (Colab 3)
   - Connection to o1 and R1

2. **GRPO Mechanism (3 min)**
   - How GRPO works
   - Group-relative rewards
   - Multiple generation strategy
   - Automatic reward assignment
   - No human labels needed

3. **Dataset & Configuration (3 min)**
   - Problem dataset structure
   - Example problems
   - GRPO hyperparameters
   - Generation settings
   - Reward function

4. **Training Process (4 min)**
   - Show training execution
   - Multiple generations per problem
   - Reward distribution
   - Metrics interpretation
   - Training curves

5. **Reasoning Demonstrations (3 min)**
   - Base model outputs (no reasoning)
   - GRPO model outputs (with reasoning)
   - Step-by-step thinking
   - Accuracy improvements
   - Reasoning quality analysis

**Total Duration:** 15 minutes

---

## üéØ Learning Outcomes

After completing this colab, you should understand:

1. What reasoning models are and why they're important
2. How GRPO enables reasoning without human labels
3. The difference between GRPO and traditional RL
4. How to generate and evaluate reasoning traces
5. How to configure and train reasoning models
6. The connection to models like o1 and R1

---

## üìä Reasoning Capability Assessment

### Quantitative Improvements
- Problem Accuracy: 40-50% ‚Üí 65-80%
- Reasoning Steps: 0 ‚Üí 3-7 per problem
- Solution Quality: Low ‚Üí High
- Self-Correction: Rare ‚Üí Common

### Qualitative Improvements
- Explicit step-by-step thinking
- Logical coherence
- Error detection and correction
- Clear explanations
- Verifiable reasoning process

### Comparison with o1/R1
- Similar chain-of-thought approach
- Automatic reasoning generation
- Improved accuracy on hard problems
- Interpretable decision making
- Scalable training method

---

**Last Updated:** November 2025
**Colab:** 4 of 5
**Previous:** Colab 3 - Reinforcement Learning
**Next:** Colab 5 - Continued Pretraining
